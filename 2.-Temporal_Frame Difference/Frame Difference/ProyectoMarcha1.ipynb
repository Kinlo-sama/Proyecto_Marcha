{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Concepto General**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame Differencing pertence al tipo de técnicas conocidas como \"Temporal/Frame Difference\" (Diferencia Temporal o Diferencia de Cuadros). Este tipo de tecnica se basan en la comparacion de cuadros consecutivos en una secuencia de video para detectar cambios que indicar movimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Concepto**\n",
    "\n",
    "La diferencia temporal se refiere a la ténica que utiliza la comparación de cuadros en diferentes momentos para detectar cambios en una secuencia de video. El principio basico es que los cambios en la escena entre dos cuadros consecutivos indican movimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pasos clave**\n",
    "\n",
    "1.- Captura de cuadros: Se capturan cuadros consecutivos de la secuencia de video.\n",
    "\n",
    "2.- Cálculo de la diferencia: Se calcula la diferencia en los valores de los pixeles entres los cuadros consecutivos \n",
    "\n",
    "3.- Umbralizacion: Se aplica un umbral para distinguir entre cambios significativos (movimiento) y ruido\n",
    "\n",
    "4.- Generación de mascara de movimiento: Se genera una mascara binaria que indica las areas de movimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ventajas**\n",
    "<ul>\n",
    "    <li>Simplificidad\n",
    "    <li>Rapidez \n",
    "    <li>Eficiencia en Escenas Estáticas\n",
    "</ul>\n",
    "\n",
    "### **Desventajas**\n",
    "<ul>\n",
    "    <li>Sensibilidad al ruido\n",
    "    <li>Problemas con iluminación \n",
    "    <li>Occlusión y sombras\n",
    "</ul>\n",
    "\n",
    "### **Ejemplos de uso**\n",
    "<ul>\n",
    "    <li>Vigilancia de seguridad\n",
    "    <li>Deteccion de Tráfico\n",
    "    <li>Aplicaciones medica\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezaremos utilizando el modelo de reconomiento de movimiento conocido como \"Frame Differencing\" de la siguiente manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 21:02:24.796190: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-31 21:02:24.823539: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-31 21:02:24.833691: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-31 21:02:24.852527: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-31 21:02:26.130878: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Librerias necesarias\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Para guardar el video usando un modelo de diferencias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_significant_bits(image, num_bits=4):\n",
    "    shift_amount = 8 - num_bits\n",
    "    return (image >> shift_amount) << shift_amount\n",
    "\n",
    "def get_least_significant_bits(image, num_bits=4):\n",
    "    mask = (1 << num_bits) - 1\n",
    "    return image & mask\n",
    "\n",
    "#Abre el video \n",
    "ruta = '../../Persona8/persona8_9_f.webm'\n",
    "cap = cv2.VideoCapture(ruta)\n",
    "#Lee el primer cuadro del video \n",
    "ret, frame = cap.read()\n",
    "#convertimos la imagen a escala de grises\n",
    "if not ret:\n",
    "    print(\"No se encontro video\") \n",
    "image1 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Obtenemos las dimensiones del cuadro en escala de grises\n",
    "row, col = image1.shape\n",
    "\n",
    "#creamos una imagen en blanco con las mismas dimensiones \n",
    "res = np.zeros([row, col, 1], np.uint8)\n",
    "\n",
    "#Intensidades para la mascara binaria \n",
    "\n",
    "#obtencion de los cuadros por segundo del video \n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "#Configuracion para la escritura del video de saldia\n",
    "out  = cv2.VideoWriter(\"personaFrameDifferencing4less.avi\", cv2.VideoWriter_fourcc(*'XVID'), fps, (col, row), isColor=False)\n",
    "\n",
    "image1 =  get_least_significant_bits(image1) * (255 // ((1 << 4) - 1))\n",
    "\n",
    "while True:\n",
    "    _, image2 = cap.read()\n",
    "    if image2 is None:\n",
    "        break\n",
    "\n",
    "    image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "    image2 = get_least_significant_bits(image2) * (255 // ((1 << 4) - 1))\n",
    "    res = cv2.absdiff(image1, image2)\n",
    "    \n",
    "    cv2.imshow('image',res)\n",
    "    out.write(res)\n",
    "    image1 = image2\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar el modelo y viendo el video con un bucle sin fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 1920)\n"
     ]
    }
   ],
   "source": [
    "#Abre el video  \n",
    "ruta = \"../../Persona8/persona8_9_f.webm\"\n",
    "cap = cv2.VideoCapture(ruta)\n",
    "\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frames = []\n",
    "while cap.isOpened():\n",
    "    ret, fram = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frames.append(frame)\n",
    "\n",
    "mediana = np.median(frames, axis=0).astype(np.uint8)\n",
    "\n",
    "cap = cv2.VideoCapture(ruta)\n",
    "#Lee el primer cuadro del video \n",
    "_, frame = cap.read()\n",
    "#convertimos la imagen a escala de grises\n",
    "image1 = mediana.astype(np.uint8)\n",
    "image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "print(image1.shape)\n",
    "#Obtenemos las dimensiones del cuadro en escala de grises\n",
    "row, col = image1.shape\n",
    "\n",
    "#obtencion de los cuadros por segundo del video \n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "#Configuracion para la escritura del video de saldia\n",
    "out  = cv2.VideoWriter(\"personaFrameDifferencingMediana.avi\", cv2.VideoWriter_fourcc(*'XVID'), fps, (col, row), isColor=False)\n",
    "ancho_nuevo = 1600\n",
    "alto_nuevo = 900\n",
    "while True:\n",
    "    _, image2 = cap.read()\n",
    "    if image2 is None:\n",
    "        #cap = cv2.VideoCapture(ruta)\n",
    "        #Lee el primer cuadro del video \n",
    "        #_, image2 = cap.read()\n",
    "        break\n",
    "    image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "    # Normalizar las imágenes\n",
    "\n",
    "    # Calcular la diferencia\n",
    "    res = cv2.absdiff(image2, image1)\n",
    "\n",
    "    # Escalar la diferencia de vuelta al rango de 0 a 255\n",
    "    out.write(res)\n",
    "    res = cv2.resize(res, (ancho_nuevo, alto_nuevo) )\n",
    "    cv2.imshow('image',res)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Usando mediapipe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1722323425.764294   28133 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1722323425.765852   28216 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 22.3.6), renderer: Mesa Intel(R) HD Graphics 530 (SKL GT2)\n",
      "W0000 00:00:1722323425.881641   28207 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1722323425.900571   28212 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "ruta = \"../../Persona8/persona8_9_f.webm\"\n",
    "cap = cv2.VideoCapture(ruta)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    cv2.imshow('Pose Estimation', image)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encontrando contorno con respecto a colores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = \"../../Persona8/persona8_9_f.webm\"\n",
    "cap = cv2.VideoCapture(ruta)\n",
    "\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, image = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Aplicar un umbral para obtener una imagen binaria\n",
    "    _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Encontrar contornos\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Crear una máscara vacía\n",
    "    mask = np.zeros_like(gray)\n",
    "\n",
    "    # Dibujar los contornos en la máscara\n",
    "    cv2.drawContours(mask, contours, -1, (255), thickness=cv2.FILLED)\n",
    "\n",
    "    # Aplicar la máscara a la imagen original\n",
    "    result = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "    # Mostrar la imagen resultante\n",
    "    cv2.imshow('Silueta', result)\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722478747.604356   54740 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1722478747.608639   56168 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 22.3.6), renderer: Mesa Intel(R) HD Graphics 530 (SKL GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1722478747.709759   56153 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1722478747.739855   56158 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/home/kinlo/.local/lib/python3.11/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuración de MediaPipe\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Inicializar el modelo de pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Cargar el video\n",
    "video_path = \"../../Persona8/persona8_9_f.webm\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "while True:\n",
    "    # Leer el siguiente cuadro del video\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Si no se puede leer más cuadros, salir del bucle\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convertir la imagen de BGR a RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Realizar la detección de pose\n",
    "    results = pose.process(rgb_frame)\n",
    "\n",
    "    # Crear una máscara negra del mismo tamaño que el cuadro\n",
    "    mask = np.zeros_like(frame)\n",
    "\n",
    "    # Dibujar las marcas y conexiones de la pose\n",
    "    if results.pose_landmarks:\n",
    "        # Obtener puntos clave y convertirlos en coordenadas\n",
    "        h, w, _ = frame.shape\n",
    "        points = []\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "            points.append((x, y))\n",
    "        \n",
    "        # Definir las conexiones entre los puntos (de la pose)\n",
    "        connections = [\n",
    "            (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER),\n",
    "            (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_ELBOW),\n",
    "            (mp_pose.PoseLandmark.RIGHT_ELBOW, mp_pose.PoseLandmark.RIGHT_WRIST),\n",
    "            (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_ELBOW),\n",
    "            (mp_pose.PoseLandmark.LEFT_ELBOW, mp_pose.PoseLandmark.LEFT_WRIST),\n",
    "            (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_HIP),\n",
    "            (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_HIP),\n",
    "            (mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP),\n",
    "            (mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.LEFT_KNEE),\n",
    "            (mp_pose.PoseLandmark.RIGHT_HIP, mp_pose.PoseLandmark.RIGHT_KNEE),\n",
    "            (mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.LEFT_ANKLE),\n",
    "            (mp_pose.PoseLandmark.RIGHT_KNEE, mp_pose.PoseLandmark.RIGHT_ANKLE),\n",
    "        ]\n",
    "\n",
    "        for start, end in connections:\n",
    "            start_point = points[start.value]\n",
    "            end_point = points[end.value]\n",
    "            cv2.line(mask, start_point, end_point, (255, 255, 255), 5)\n",
    "        \n",
    "        # Crear una máscara de relleno usando un relleno convexo\n",
    "        points_array = np.array(points, np.int32)\n",
    "        if len(points_array) > 0:\n",
    "            cv2.fillConvexPoly(mask, points_array, (255, 255, 255))\n",
    "\n",
    "    # Aplicar la máscara al cuadro original\n",
    "    result = cv2.bitwise_and(frame, mask)\n",
    "\n",
    "    # Mostrar el resultado\n",
    "    cv2.imshow('Silueta', result)\n",
    "\n",
    "    # Salir del bucle si se presiona 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberar los recursos\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to implement  \n",
    "# Webcam Motion Detector \n",
    "  \n",
    "# importing OpenCV, time and Pandas library \n",
    "import cv2, time, pandas \n",
    "# importing datetime class from datetime library \n",
    "from datetime import datetime \n",
    "  \n",
    "# Assigning our static_back to None \n",
    "static_back = None\n",
    "  \n",
    "# List when any moving object appear \n",
    "motion_list = [ None, None ] \n",
    "\n",
    "video_path = \"../../Persona8/persona8_9_f.webm\"\n",
    "# Time of movement \n",
    "time = [] \n",
    "  \n",
    "# Initializing DataFrame, one column is start  \n",
    "# time and other column is end time \n",
    "df = pandas.DataFrame(columns = [\"Start\", \"End\"]) \n",
    "  \n",
    "# Capturing video \n",
    "video = cv2.VideoCapture(video_path) \n",
    "  \n",
    "# Infinite while loop to treat stack of image as video \n",
    "while True: \n",
    "    # Reading frame(image) from video \n",
    "    check, frame = video.read() \n",
    "    if not check:\n",
    "        break\n",
    "  \n",
    "    # Initializing motion = 0(no motion) \n",
    "    motion = 0\n",
    "  \n",
    "    # Converting color image to gray_scale image \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) \n",
    "  \n",
    "    # Converting gray scale image to GaussianBlur  \n",
    "    # so that change can be find easily \n",
    "    gray = cv2.GaussianBlur(gray, (21, 21), 0) \n",
    "  \n",
    "    # In first iteration we assign the value  \n",
    "    # of static_back to our first frame \n",
    "    if static_back is None: \n",
    "        static_back = gray \n",
    "        continue\n",
    "  \n",
    "    # Difference between static background  \n",
    "    # and current frame(which is GaussianBlur) \n",
    "    diff_frame = cv2.absdiff(static_back, gray) \n",
    "  \n",
    "    # If change in between static background and \n",
    "    # current frame is greater than 30 it will show white color(255) \n",
    "    thresh_frame = cv2.threshold(diff_frame, 30, 255, cv2.THRESH_BINARY)[1] \n",
    "    thresh_frame = cv2.dilate(thresh_frame, None, iterations = 2) \n",
    "  \n",
    "    # Finding contour of moving object \n",
    "    cnts,_ = cv2.findContours(thresh_frame.copy(),  \n",
    "                       cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) \n",
    "  \n",
    "    for contour in cnts: \n",
    "        if cv2.contourArea(contour) < 10000: \n",
    "            continue\n",
    "        motion = 1\n",
    "  \n",
    "        (x, y, w, h) = cv2.boundingRect(contour) \n",
    "        # making green rectangle around the moving object \n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3) \n",
    "  \n",
    "    # Appending status of motion \n",
    "    motion_list.append(motion) \n",
    "  \n",
    "    motion_list = motion_list[-2:] \n",
    "  \n",
    "    # Appending Start time of motion \n",
    "    if motion_list[-1] == 1 and motion_list[-2] == 0: \n",
    "        time.append(datetime.now()) \n",
    "  \n",
    "    # Appending End time of motion \n",
    "    if motion_list[-1] == 0 and motion_list[-2] == 1: \n",
    "        time.append(datetime.now()) \n",
    "  \n",
    "    # Displaying image in gray_scale \n",
    "    #cv2.imshow(\"Gray Frame\", gray) \n",
    "  \n",
    "    # Displaying the difference in currentframe to \n",
    "    # the staticframe(very first_frame) \n",
    "    #cv2.imshow(\"Difference Frame\", diff_frame) \n",
    "  \n",
    "    # Displaying the black and white image in which if \n",
    "    # intensity difference greater than 30 it will appear white \n",
    "    cv2.imshow(\"Threshold Frame\", thresh_frame) \n",
    "  \n",
    "    # Displaying color frame with contour of motion of object \n",
    "    #cv2.imshow(\"Color Frame\", frame) \n",
    "    # if q entered whole process will stop \n",
    "    # Salir del bucle si se presiona 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "  \n",
    "  \n",
    "video.release() \n",
    "  \n",
    "# Destroying all the windows \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Binarizacion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'personaFrameDifferencingMediana.avi'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "umbral = 60\n",
    "ret, frame = cap.read()\n",
    "\n",
    "if not ret:\n",
    "    print(\"hola\")\n",
    "\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "out = cv2.VideoWriter(\"personaFrameDifferencingBinari.avi\", cv2.VideoWriter_fourcc(*'XVID'), fps, (width, height), isColor=False)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    if len(frame.shape) == 3:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_frame = cv2.threshold(frame, umbral, 255, cv2.THRESH_BINARY)\n",
    "    out.write(binary_frame)\n",
    "    cv2.imshow('Video binarizado', binary_frame)\n",
    "\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break \n",
    "\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Morphological Transformations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero empezando con **Opening** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((5,5), np.uint8)\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2,2))\n",
    "video_path = \"personaFrameDifferencingBinari.avi\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "out = cv2.VideoWriter(\"personaFrameDifferencingOpeningEclipse_2.avi\", cv2.VideoWriter_fourcc(*'XVID'), fps, (width, height), isColor=False)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if len(frame.shape) == 3:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    opening = cv2.morphologyEx(frame, cv2.MORPH_OPEN, kernel=kernel)\n",
    "    cv2.imshow('Video binarizado', opening)\n",
    "    out.write(opening)\n",
    "\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break \n",
    "\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora con el **Closing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((100, 100), np.uint8)\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (50,50))\n",
    "video_path = \"personaFrameDifferencingOpening.avi\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "out = cv2.VideoWriter(\"personaFrameDifferencingClosingCross50_50.avi\", cv2.VideoWriter_fourcc(*'XVID'), fps, (width, height), isColor=False)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if len(frame.shape) == 3:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    closing = cv2.morphologyEx(frame, cv2.MORPH_CLOSE, kernel=kernel)\n",
    "    cv2.imshow('Video', closing)\n",
    "    out.write(closing)\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break \n",
    "\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Edges con Canny**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abre el video \n",
    "ruta = \"../../Persona8/persona8_9_f.webm\"\n",
    "cap = cv2.VideoCapture(ruta)\n",
    "#Lee el primer cuadro del video \n",
    "_, frame = cap.read()\n",
    "#convertimos la imagen a escala de grises\n",
    "image1 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Obtenemos las dimensiones del cuadro en escala de grises\n",
    "row, col = image1.shape\n",
    "\n",
    "#creamos una imagen en blanco con las mismas dimensiones \n",
    "res = np.zeros([row, col, 1], np.uint8)\n",
    "\n",
    "#Intensidades para la mascara binaria \n",
    "\n",
    "#obtencion de los cuadros por segundo del video \n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "#Configuracion para la escritura del video de saldia\n",
    "out  = cv2.VideoWriter(\"personaFrameEdges.avi\", cv2.VideoWriter_fourcc(*'XVID'), fps, (col, row), isColor=False)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    edges = cv2.Canny(frame, 5, 50)\n",
    "\n",
    "    cv2.imshow('Edges', edges)\n",
    "    out.write(edges)\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows(  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Usando Sobel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abre el video \n",
    "ruta = \"../../Persona8/persona8_9_f.webm\"\n",
    "cap = cv2.VideoCapture(ruta)\n",
    "#Lee el primer cuadro del video \n",
    "_, frame = cap.read()\n",
    "#convertimos la imagen a escala de grises\n",
    "image1 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Obtenemos las dimensiones del cuadro en escala de grises\n",
    "row, col = image1.shape\n",
    "\n",
    "#creamos una imagen en blanco con las mismas dimensiones \n",
    "res = np.zeros([row, col, 1], np.uint8)\n",
    "\n",
    "#Intensidades para la mascara binaria \n",
    "\n",
    "#obtencion de los cuadros por segundo del video \n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "#Configuracion para la escritura del video de saldia\n",
    "out  = cv2.VideoWriter(\"personaFrameEdges.avi\", cv2.VideoWriter_fourcc(*'XVID'), fps, (col, row), isColor=False)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame_blur = cv2.GaussianBlur(frame_gray, (5,5), 0)\n",
    "\n",
    "    laplacian = cv2.Laplacian(frame_blur, cv2.CV_64F)\n",
    "    cv2.imshow('Edge sobely', laplacian)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Contour Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def img_to_polygons(image):\n",
    "    cordnt_list = []\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply thresholding to create a binary image\n",
    "    _, th2 = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Find contours in the binary image\n",
    "    contours, hierarchy = cv2.findContours(th2, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    # Extract coordinates of white regions\n",
    "    white_regions_coordinates = []\n",
    "    for contour in contours:\n",
    "        coordinates = contour.squeeze().astype(float).tolist()\n",
    "        white_regions_coordinates.append(coordinates)\n",
    "\n",
    "    # Print the coordinates of each white region\n",
    "    for i, region in enumerate(white_regions_coordinates):\n",
    "        if type(region[0]) is list:\n",
    "            if len(region) > 2:\n",
    "                # Calculate the area of the contour\n",
    "                area = cv2.contourArea(np.around(np.array([[pnt] for pnt in region])).astype(np.int32))\n",
    "                if area > 100:\n",
    "                    # Convert coordinates to the required format\n",
    "                    crdnts = [{'x': i[0], 'y': i[1]} for i in region]\n",
    "                    cordnt_list.append(crdnts)\n",
    "\n",
    "    return cordnt_list\n",
    "\n",
    "#Abre el video \n",
    "ruta = \"../../Persona8/persona8_9_f.webm\"\n",
    "cap = cv2.VideoCapture(ruta)\n",
    "#Lee el primer cuadro del video \n",
    "ret, frame = cap.read()\n",
    "#convertimos la imagen a escala de grises\n",
    "if ret:\n",
    "    image1 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #Obtenemos las dimensiones del cuadro en escala de grises\n",
    "    row, col = image1.shape\n",
    "\n",
    "    #creamos una imagen en blanco con las mismas dimensiones \n",
    "    res = np.zeros([row, col, 1], np.uint8)\n",
    "\n",
    "    #Intensidades para la mascara binaria \n",
    "\n",
    "    #obtencion de los cuadros por segundo del video \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    out  = cv2.VideoWriter(\"personaFrameEdgesContor.avi\", cv2.VideoWriter_fourcc(*'XVID'), fps, (col, row), isColor=False)\n",
    "\n",
    "else:\n",
    "    print(\"Erorr\")\n",
    "\n",
    "\n",
    "# Process each file in the input directory\n",
    "while cap.isOpened():\n",
    "    # Input and output file paths\n",
    "\n",
    "    ret, img = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # Extract polygons from the image\n",
    "    polygons = img_to_polygons(img)\n",
    "\n",
    "    # Get the height and width of the image\n",
    "    height, width = img.shape[:2]\n",
    "\n",
    "    # Create a black image with the same dimensions as the input\n",
    "    black_image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    # Draw polygons on the black image\n",
    "    for ply in polygons:\n",
    "        ply_list = [[pnt['x'], pnt['y']] for pnt in ply]\n",
    "\n",
    "        # Define the vertices of the polygon\n",
    "        vertices = np.array(ply_list, dtype=np.int32)\n",
    "\n",
    "        # Draw the polygon on the black image\n",
    "        cv2.polylines(black_image, [vertices], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "    cv2.imshow('Poligonos', black_image)\n",
    "    out.write(black_image)\n",
    "    # Salir del bucle con la tecla 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "out.release()\n",
    "# Liberar los recursos\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoEdge = \"personaFrameEdges.avi\"\n",
    "videoClose = \"personaFrameDifferencingClosingCross50_50.avi\"\n",
    "\n",
    "capEdge = cv2.VideoCapture(videoEdge)\n",
    "capClose = cv2.VideoCapture(videoClose)\n",
    "\n",
    "width = int(capEdge.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(capEdge.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps= int(capEdge.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('FinalEdge.mp4', fourcc, fps, (width, height))\n",
    "\n",
    "while capEdge.isOpened():\n",
    "    ret1, frame1 = capEdge.read()\n",
    "    ret2, frame2 = capClose.read()\n",
    "\n",
    "    if not ret1 or not ret2:\n",
    "        break\n",
    "\n",
    "    frame1 = cv2.resize(frame1, (width, height))\n",
    "    frame2 = cv2.resize(frame2, (width, height))\n",
    "\n",
    "    bitwise_and = cv2.bitwise_and(frame1, frame2)\n",
    "    out.write(bitwise_and)\n",
    "    cv2.imshow(\"Bit_and\", bitwise_and)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "out.release()\n",
    "capEdge.release()\n",
    "capClose.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_significant_bits(image, num_bits=4):\n",
    "    # Reduce to the most significant bits\n",
    "    shift_amount = 8 - num_bits\n",
    "    return (image >> shift_amount) << shift_amount\n",
    "\n",
    "def get_least_significant_bits(image, num_bits=4):\n",
    "    # Crear una máscara para los bits menos significativos\n",
    "    mask = (1 << num_bits) - 1\n",
    "    return image & mask\n",
    "\n",
    "#Abre el video \n",
    "ruta = '../../Persona8/persona8_9_f.webm'\n",
    "cap = cv2.VideoCapture(ruta)\n",
    "#Lee el primer cuadro del video \n",
    "ret, frame = cap.read()\n",
    "#convertimos la imagen a escala de grises\n",
    "if not ret:\n",
    "    print(\"No se encontro video\") \n",
    "image1 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Obtenemos las dimensiones del cuadro en escala de grises\n",
    "row, col = image1.shape\n",
    "\n",
    "#creamos una imagen en blanco con las mismas dimensiones \n",
    "res = np.zeros([row, col, 1], np.uint8)\n",
    "\n",
    "#Intensidades para la mascara binaria \n",
    "\n",
    "#obtencion de los cuadros por segundo del video \n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "#Configuracion para la escritura del video de saldia\n",
    "#out  = cv2.VideoWriter(\"personaFrameDifferencing4less.avi\", cv2.VideoWriter_fourcc(*'XVID'), fps, (col, row), isColor=False)\n",
    "\n",
    "\n",
    "image1 =  get_least_significant_bits(image1) * (255 // ((1 << 4) - 1))\n",
    "\n",
    "\n",
    "cv2.imshow(\"imagen\", image1)\n",
    "cv2.imwrite(\"4LSB.png\",image1)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
